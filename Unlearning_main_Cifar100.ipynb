{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEzf9jIxeRUn",
    "outputId": "acf8db20-a871-401b-cab0-84f3544ddeb2"
   },
   "outputs": [],
   "source": [
    "drive_folder = \"Machine_Unlearning_Drive/Cifar100Results/\"\n",
    "\n",
    "ssd_folder = \"SSD/\"\n",
    "\n",
    "scrub_folder = \"SCRUB/\"\n",
    "\n",
    "github_folder = \"Machine_Unlearning/\"\n",
    "\n",
    "!pip install scikit-learn torch torchvision seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRSBGzBxjoq-",
    "outputId": "5919a0f1-5d99-40d6-ec9c-c8d93ba095e7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "import random\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from Machine_Unlearning.Metrics.metrics import *\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "def seed_everything(seed):\n",
    "  RNG = torch.Generator().manual_seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  return RNG\n",
    "\n",
    "SEED = 44\n",
    "RNG = seed_everything(SEED)\n",
    "SPLIT = 0.3\n",
    "STARTING_ALPHA = SPLIT * 5/3\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-qMvTmwjrnA",
    "outputId": "23b368d2-1a0f-4259-fa26-ef4809129b2a"
   },
   "outputs": [],
   "source": [
    "# download and pre-process CIFAR10\n",
    "normalize = transforms.Compose(\n",
    "    [\n",
    "      transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100(\n",
    "    root=\"./data\", train=True, download=True, transform=normalize\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "# we split held out data into test and validation set\n",
    "held_out = torchvision.datasets.CIFAR100(\n",
    "    root=\"./data\", train=False, download=True, transform=normalize\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "GEN1 = torch.Generator().manual_seed(42)\n",
    "retain_set, forget_set = torch.utils.data.random_split(train_set,[1-SPLIT,SPLIT],GEN1)\n",
    "\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=256, shuffle=True, num_workers=2 , generator=RNG\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzhrtkVWqbxj"
   },
   "outputs": [],
   "source": [
    "def readout(model,name):\n",
    "  RNG = seed_everything(SEED)\n",
    "  test_entropies = compute_entropy(model, test_loader)\n",
    "  retain_entropies = compute_entropy(model, retain_loader)\n",
    "  forget_entropies = compute_entropy(model, forget_loader)\n",
    "\n",
    "\n",
    "  results[f\"test_entropies_{name}\"] = test_entropies.tolist()\n",
    "  results[f\"retain_entropies_{name}\"] = retain_entropies.tolist()\n",
    "  results[f\"forget_entropies_{name}\"] = forget_entropies.tolist()\n",
    "\n",
    "  test_losses = compute_losses(model, test_loader)\n",
    "  retain_losses = compute_losses(model, retain_loader)\n",
    "  forget_losses = compute_losses(model, forget_loader)\n",
    "\n",
    "  results[f\"test_losses_{name}\"] = test_losses.tolist()\n",
    "  results[f\"retain_losses_{name}\"] = retain_losses.tolist()\n",
    "  results[f\"forget_losses_{name}\"] = forget_losses.tolist()\n",
    "\n",
    "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
    "  gen = np.random.default_rng(1)\n",
    "  if len(test_losses) > len(forget_losses):\n",
    "    gen.shuffle(test_losses)\n",
    "    test_losses = test_losses[: len(forget_losses)]\n",
    "  else:\n",
    "    gen.shuffle(forget_losses)\n",
    "    forget_losses = forget_losses[: len(test_losses)]\n",
    "    # make sure we have a balanced dataset for the MIA\n",
    "  assert len(test_losses) == len(forget_losses)\n",
    "\n",
    "  samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
    "  labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
    "\n",
    "  mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "\n",
    "  print(\n",
    "      f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "  )\n",
    "\n",
    "  results[f\"MIA_losses_{name}\"] = mia_scores.mean()\n",
    "\n",
    "  gen = np.random.default_rng(1)\n",
    "  if len(test_entropies) > len(forget_entropies):\n",
    "    gen.shuffle(test_entropies)\n",
    "    test_entropies = test_entropies[: len(forget_entropies)]\n",
    "  else:\n",
    "    gen.shuffle(forget_entropies)\n",
    "    forget_entropies = forget_entropies[: len(test_entropies)]\n",
    "    # make sure we have a balanced dataset for the MIA\n",
    "  assert len(test_entropies) == len(forget_entropies)\n",
    "\n",
    "  samples_mia = np.concatenate((test_entropies, forget_entropies)).reshape((-1, 1))\n",
    "  labels_mia = [0] * len(test_entropies) + [1] * len(forget_entropies)\n",
    "\n",
    "  mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "\n",
    "  print(\n",
    "      f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "  )\n",
    "\n",
    "  results[f\"MIA_entropies_{name}\"] = mia_scores.mean()\n",
    "\n",
    "  results[f\"train_accuracy_{name}\"] = accuracy(model, retain_loader)\n",
    "  results[f\"test_accuracy_{name}\"] = accuracy(model, test_loader)\n",
    "  results[f\"forget_accuracy_{name}\"] = accuracy(model, forget_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmw8VtI-kkgh",
    "outputId": "7392705a-31d1-43bc-c834-623ed8e21a08"
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model(DEVICE):\n",
    "\n",
    "  local_path = github_folder + \"CIFAR100_Resnet18.ckp\"\n",
    "  weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "  # load model with pre-trained weights\n",
    "  model = resnet18(weights=None, num_classes=100)\n",
    "  model.load_state_dict(weights_pretrained)\n",
    "  model.to(DEVICE)\n",
    "  model.eval();\n",
    "  return model\n",
    "\n",
    "model = load_pretrained_model(DEVICE)\n",
    "readout(model,\"original\")\n",
    "print(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laJfK5goD7NI"
   },
   "outputs": [],
   "source": [
    "def unlearning(net, retain, forget, validation, start_alpha = 0.1, alpha_sched = lambda start_a,a,max_ep,ep: a-(start_a/max_ep), lr = 0.001, forget_epochs = 27, use_scheduler = True):\n",
    "    import math\n",
    "\n",
    "\n",
    "    ### FORGETTING ###\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = optim.AdamW(net.parameters(), lr= lr )\n",
    "    forget_epochs = forget_epochs\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=forget_epochs)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    alpha = start_alpha\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    retain_iter = iter(retain)\n",
    "\n",
    "    def entropy(outputs):\n",
    "\n",
    "        p = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "        return (-torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=-1, keepdim=False))\n",
    "\n",
    "    for i in range(forget_epochs):\n",
    "      net.eval()\n",
    "\n",
    "      if i%5==0 :\n",
    "        print(\"Computing current moments on test set\")\n",
    "        val_loss, first_test_moment, second_test_moment, test_std = compute_moments(net, validation)\n",
    "        #train_mean, first_train_moment, second_train_moment,train_std = compute_moments(net,retain_loader)\n",
    "        print(\"Computed moments: \"+str(val_loss)+\",\"+str(first_test_moment)+\",\"+str(second_test_moment))\n",
    "\n",
    "\n",
    "      ft_forget_losses = compute_losses(net, forget)\n",
    "      ft_test_losses = compute_losses(net, test_loader)\n",
    "\n",
    "      gen = np.random.default_rng(1)\n",
    "\n",
    "      if len(ft_test_losses) > len(ft_forget_losses):\n",
    "        gen.shuffle(ft_test_losses)\n",
    "        ft_test_losses = ft_test_losses[: len(ft_forget_losses)]\n",
    "      else:\n",
    "        gen.shuffle(ft_forget_losses)\n",
    "        ft_forget_losses = ft_forget_losses[: len(ft_test_losses)]\n",
    "\n",
    "      # make sure we have a balanced dataset for the MIA\n",
    "      assert len(ft_test_losses) == len(ft_forget_losses)\n",
    "\n",
    "      ft_samples_mia = np.concatenate((ft_test_losses, ft_forget_losses)).reshape((-1, 1))\n",
    "      labels_mia = [0] * len(ft_test_losses) + [1] * len(ft_forget_losses)\n",
    "\n",
    "      ft_mia_scores = simple_mia(ft_samples_mia, labels_mia)\n",
    "\n",
    "      print(\n",
    "          f\"The MIA has an accuracy of {ft_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "      )\n",
    "      mia_metric_scores.append(ft_mia_scores.mean())\n",
    "\n",
    "      acc = 100.0 * accuracy(net, test_loader)\n",
    "      print(f\"Accuracy on test set: {acc:.1f} \")\n",
    "      accuracy_metric_scores.append(acc)\n",
    "\n",
    "\n",
    "      net.eval()\n",
    "\n",
    "      print(\"Forgetting epoch \"+str(i))\n",
    "\n",
    "      if i % (len(retain)//len(forget))==0:\n",
    "        print(\"Resetting retain iterator...\")\n",
    "        retain_iter = iter(retain)\n",
    "\n",
    "\n",
    "      print(\"using alpha: \"+str(alpha))\n",
    "      for c , (inputs, targets) in enumerate(forget):\n",
    "\n",
    "\n",
    "        net.zero_grad()\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        out = net(inputs)\n",
    "\n",
    "        r_inputs, r_targets = next(retain_iter)\n",
    "        r_inputs, r_targets = r_inputs.to(DEVICE), r_targets.to(DEVICE)\n",
    "        r_out = net(r_inputs)\n",
    "\n",
    "\n",
    "        forget_losses = criterion(out, targets)\n",
    "        retain_losses = criterion(r_out,r_targets)\n",
    "\n",
    "        #Forget loss metrics\n",
    "        forget_mean = torch.mean(forget_losses)\n",
    "        #print(forget_mean)\n",
    "        forget_var = torch.mean((forget_losses-forget_mean)**2)\n",
    "        forget_std = forget_var**0.5\n",
    "        forget_skew = torch.mean((forget_losses-forget_mean)**3) / (forget_std**3)\n",
    "\n",
    "        delta_val_loss =  (val_loss - forget_mean)\n",
    "        delta_first_moment = (first_test_moment - forget_var)\n",
    "        delta_second_moment = (second_test_moment - forget_skew)\n",
    "\n",
    "        #Retain loss metric\n",
    "        retain_mean = torch.mean(retain_losses)\n",
    "\n",
    "\n",
    "        if c % 40 == 0:\n",
    "          print(\"delta_val_loss: \"+str(delta_val_loss.item()))\n",
    "          print(\"delta_first_moment: \"+str(delta_first_moment.item()))\n",
    "          print(\"delta_second_moment: \"+str(delta_second_moment.item()))\n",
    "\n",
    "        loss =  alpha*(torch.nn.functional.relu(delta_val_loss)**2) + (1-alpha)* retain_mean\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      alpha = alpha_sched(start_alpha,alpha,forget_epochs,i)\n",
    "      if use_scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    net.eval()\n",
    "    ft_forget_losses = compute_losses(net, forget_loader)\n",
    "    ft_test_losses = compute_losses(net, test_loader)\n",
    "\n",
    "    gen = np.random.default_rng(1)\n",
    "\n",
    "    if len(ft_test_losses) > len(ft_forget_losses):\n",
    "      gen.shuffle(ft_test_losses)\n",
    "      ft_test_losses = ft_test_losses[: len(ft_forget_losses)]\n",
    "    else:\n",
    "      gen.shuffle(ft_forget_losses)\n",
    "      ft_forget_losses = ft_forget_losses[: len(ft_test_losses)]\n",
    "    # make sure we have a balanced dataset for the MIA\n",
    "    assert len(ft_test_losses) == len(ft_forget_losses)\n",
    "\n",
    "    ft_samples_mia = np.concatenate((ft_test_losses, ft_forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(ft_test_losses) + [1] * len(ft_forget_losses)\n",
    "\n",
    "    ft_mia_scores = simple_mia(ft_samples_mia, labels_mia)\n",
    "\n",
    "    print(\n",
    "        f\"The MIA has an accuracy of {ft_mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "    )\n",
    "\n",
    "    mia_metric_scores.append(ft_mia_scores.mean())\n",
    "\n",
    "    acc = 100.0 * accuracy(net, test_loader)\n",
    "    print(f\"Accuracy on test set: {acc:.1f} \")\n",
    "    accuracy_metric_scores.append(acc)\n",
    "\n",
    "    net.eval()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7AZPHqOlSpQ",
    "outputId": "a90c9205-e850-4b8a-d106-18d1c7d6198a"
   },
   "outputs": [],
   "source": [
    "RNG = seed_everything(SEED)\n",
    "\n",
    "ft_model = load_pretrained_model(DEVICE)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=256, shuffle=True, num_workers=2 , generator=RNG\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
    ")\n",
    "# Execute the unlearing routine. This might take a few minutes.\n",
    "# If run on colab, be sure to be running it on  an instance with GPUs\n",
    "accuracy_metric_scores = []\n",
    "mia_metric_scores = []\n",
    "delta_forget_losses = []\n",
    "delta_val_losses = []\n",
    "reg_factors = []\n",
    "print(len(train_loader))\n",
    "print(len(retain_loader))\n",
    "print(len(forget_loader))\n",
    "\n",
    "def alpha_sched(start_a,a,max_ep,ep):\n",
    "  #return start_a\n",
    "  #if ep > 10:\n",
    "    #return a - (start_a/(max_ep-10))\n",
    "  #else:\n",
    "   # return a\n",
    "  #return start_a /(ep+1)\n",
    "  return a - (start_a/(max_ep))\n",
    "\n",
    "#TODO: Relu with other parameters\n",
    "#TODO: Relu with entropy\n",
    "forget_epochs = int((len(retain_loader) / (len(forget_loader)*2)) * 6)\n",
    "print(forget_epochs)\n",
    "ft_model = unlearning(ft_model, retain_loader, forget_loader, val_loader,start_alpha=STARTING_ALPHA, alpha_sched=alpha_sched, lr = 0.001, forget_epochs = forget_epochs, use_scheduler = True)\n",
    "\n",
    "results[\"mia_scores_ours\"] = mia_metric_scores\n",
    "results[\"accuracy_scores_ours\"] = accuracy_metric_scores\n",
    "\n",
    "readout(ft_model,\"ours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdFXY_6tL8Tz",
    "outputId": "a38d9ed2-0e7e-45f5-e16a-46754726368d"
   },
   "outputs": [],
   "source": [
    "RNG = seed_everything(SEED)\n",
    "\n",
    "finetuning_model = load_pretrained_model(DEVICE)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=256, shuffle=True, num_workers=2 , generator=RNG\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
    ")\n",
    "# Execute the unlearing routine. This might take a few minutes.\n",
    "# If run on colab, be sure to be running it on  an instance with GPUs\n",
    "accuracy_metric_scores = []\n",
    "mia_metric_scores = []\n",
    "\n",
    "print(len(train_loader))\n",
    "\n",
    "\n",
    "print(len(forget_loader))\n",
    "\n",
    "def alpha_sched(start_a,a,max_ep,ep):\n",
    "  return a - (start_a/(max_ep))\n",
    "\n",
    "#TODO: Relu with other parameters\n",
    "#TODO: Relu with entropy\n",
    "forget_epochs = int((len(retain_loader) / (len(forget_loader))) * 6)\n",
    "print(forget_epochs)\n",
    "finetuning_model = unlearning(finetuning_model, retain_loader, forget_loader, test_loader,start_alpha=0, alpha_sched=alpha_sched, lr = 0.001, forget_epochs = forget_epochs, use_scheduler = True)\n",
    "\n",
    "results[\"mia_scores_finetuning\"] = mia_metric_scores\n",
    "results[\"accuracy_scores_finetuning\"] = accuracy_metric_scores\n",
    "\n",
    "readout(finetuning_model,\"finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "1CWtnMRRok0j",
    "outputId": "aaf9ea68-9c51-410b-c276-586dc43c99ac"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Ours vs Finetuning Baseline\")\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(0.0,6.0,len(results[\"mia_scores_ours\"])),results[\"mia_scores_ours\"],label=\"Gradient reversing (Ours)\")\n",
    "plt.plot(np.linspace(0.0,6.0,len(results[\"mia_scores_finetuning\"])),results[\"mia_scores_finetuning\"],label=\"Finetuning Baseline\")\n",
    "\n",
    "plt.hlines(0.5, 0 ,6 , color=[\"red\"],linestyles='dashed',label=\"Perfect unlearning (Retraining)\")\n",
    "plt.axhspan(0.505, 0.495, facecolor=\"red\", alpha=0.2)\n",
    "\n",
    "plt.xticks(np.arange(0.2,6,0.2),minor=True)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MIA Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "9eVprV7NpGu5",
    "outputId": "13962c3c-6c27-4158-a0f3-c02735fe1a92"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Ours vs Finetuning Baseline\")\n",
    "\n",
    "plt.plot(np.linspace(0.0,6.0,len(results[\"accuracy_scores_ours\"])),results[\"accuracy_scores_ours\"],label=\"Gradient reversing (Ours)\")\n",
    "plt.plot(np.linspace(0.0,6.0,len(results[\"accuracy_scores_finetuning\"])),results[\"accuracy_scores_finetuning\"],label=\"Finetuning Baseline\")\n",
    "plt.hlines(results[\"accuracy_scores_ours\"][0], 0 ,6 , color=[\"red\"],linestyles='dashed',label=\"Perfect unlearning (Retraining)\")\n",
    "\n",
    "plt.xticks(np.arange(0.2,6,0.2),minor=True)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Test set Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "g9oxZG-sxEbi",
    "outputId": "c245679a-1790-4b10-c0a2-03e30cf9278f"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "#ax1.set_title(f\"Pre-trained model.\\nAttack accuracy: {mia_scores.mean():0.3f}\")\n",
    "ax1.hist(results[\"test_losses_ours\"], density=True, alpha=0.5, bins=50, label=\"Test set on pretrained model\")\n",
    "ax1.hist(results[\"forget_losses_ours\"], density=True, alpha=0.5, bins=50, label=\"Forget set on unlearned model\")\n",
    "#ax1.hist(test_losses, density=True, alpha=0.3, bins=50, label=\"Forget set on unlearned model\")\n",
    "\n",
    "#ax2.set_title(f\"Unlearned by fine-tuning.\\nAttack accuracy: {ft_mia_scores.mean():0.3f}\")\n",
    "ax2.hist(results[\"test_losses_original\"], density=True, alpha=0.5, bins=50, label=\"Retain set on pretrained model\")\n",
    "ax2.hist(results[\"forget_losses_original\"], density=True, alpha=0.5, bins=50, label=\"Retain set on unlearned model\")\n",
    "\n",
    "ax1.set_xlabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "#ax1.set_xlim((0, max(np.max(ft_test_losses),np.max(ft_forget_losses))))\n",
    "#ax2.set_xlim((0, max(np.max(test_losses),np.max(forget_losses))))\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "ax2.legend(frameon=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdyff2sBPA6A"
   },
   "outputs": [],
   "source": [
    "from SCRUB.thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from SCRUB.thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from SCRUB.thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from SCRUB.thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from SCRUB.thirdparty.repdistiller.helper.pretrain import init\n",
    "\n",
    "import copy\n",
    "\n",
    "!mkdir checkpoints\n",
    "\n",
    "def scrub(teacher, student):\n",
    "\n",
    "    class AttributeDict(dict):\n",
    "      __getattr__ = dict.__getitem__\n",
    "      __setattr__ = dict.__setitem__\n",
    "      __delattr__ = dict.__delitem__\n",
    "    args = AttributeDict({})\n",
    "    args['optim'] = 'sgd'\n",
    "    args['gamma'] = 1\n",
    "    args['alpha'] = 0.5\n",
    "    args['beta'] = 0\n",
    "    args['smoothing'] = 0.5\n",
    "    args['msteps'] = 3\n",
    "    args['clip'] = 0.2\n",
    "    args['sstart'] = 10\n",
    "    args['kd_T'] = 4\n",
    "    args['distill'] = 'kd'\n",
    "\n",
    "    args['sgda_epochs'] = 6\n",
    "    args['sgda_learning_rate'] = 0.0005\n",
    "    args['lr_decay_epochs'] = [3,5,9]\n",
    "    args['lr_decay_rate'] = 0.1\n",
    "    args['sgda_weight_decay'] = 5e-4\n",
    "    args['sgda_momentum'] = 0.9\n",
    "\n",
    "    args['model'] = \"resnet18\"\n",
    "    args['dataset'] = \"cifar10\"\n",
    "    args['seed'] =  1\n",
    "\n",
    "\n",
    "    print(args)\n",
    "    print(args.clip)\n",
    "    model_t = copy.deepcopy(teacher)\n",
    "    model_s = copy.deepcopy(student)\n",
    "\n",
    "    #this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "    #For SGDA smoothing\n",
    "    beta = 0.1\n",
    "    def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "        1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "    swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "        model_s, avg_fn=avg_fn)\n",
    "\n",
    "    module_list = nn.ModuleList([])\n",
    "    module_list.append(model_s)\n",
    "    trainable_list = nn.ModuleList([])\n",
    "    trainable_list.append(model_s)\n",
    "\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_div = DistillKL(args.kd_T)\n",
    "    criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "    criterion_list = nn.ModuleList([])\n",
    "    criterion_list.append(criterion_cls)    # classification loss\n",
    "    criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "    criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "    acc_fs = []\n",
    "\n",
    "    # optimizer\n",
    "    if args.optim == \"sgd\":\n",
    "        optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                              lr=args.sgda_learning_rate,\n",
    "                              momentum=args.sgda_momentum,\n",
    "                              weight_decay=args.sgda_weight_decay)\n",
    "    elif args.optim == \"adam\":\n",
    "        optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                              lr=args.sgda_learning_rate,\n",
    "                              weight_decay=args.sgda_weight_decay)\n",
    "    elif args.optim == \"rmsp\":\n",
    "        optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                              lr=args.sgda_learning_rate,\n",
    "                              momentum=args.sgda_momentum,\n",
    "                              weight_decay=args.sgda_weight_decay)\n",
    "\n",
    "    module_list.append(model_t)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        module_list.cuda()\n",
    "        criterion_list.cuda()\n",
    "        import torch.backends.cudnn as cudnn\n",
    "        cudnn.benchmark = True\n",
    "        swa_model.cuda()\n",
    "\n",
    "    scrub_name = \"checkpoints/scrub_{}_{}_seed{}_step\".format(args.model, args.dataset, args.seed)\n",
    "    for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "        lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "        acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "        acc_fs.append(100-acc_f.item())\n",
    "\n",
    "\n",
    "        maximize_loss = 0\n",
    "        if epoch <= args.msteps:\n",
    "            maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "        train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "        if epoch >= args.sstart:\n",
    "            swa_model.update_parameters(model_s)\n",
    "\n",
    "        torch.save(model_s.state_dict(), scrub_name+str(epoch)+\".pt\")\n",
    "\n",
    "\n",
    "        print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "\n",
    "\n",
    "\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        selected_idx, _ = min(enumerate(acc_fs), key=lambda x: abs(x[1]-acc_fvs[-1]))\n",
    "    except:\n",
    "        selected_idx = len(acc_fs) - 1\n",
    "    print (\"the selected index is {}\".format(selected_idx))\n",
    "    selected_model = \"checkpoints/scrub_{}_{}_seed{}_step{}.pt\".format(args.model, args.dataset, args.seed, int(selected_idx))\n",
    "    model_s_final = copy.deepcopy(model_s)\n",
    "    model_s.load_state_dict(torch.load(selected_model))\n",
    "\n",
    "\n",
    "    return model_s, model_s_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlWsocbBRw9c",
    "outputId": "a9ca56c4-f97a-4e58-e573-ed8bb2e8814b"
   },
   "outputs": [],
   "source": [
    "RNG = seed_everything(SEED)\n",
    "teacher = load_pretrained_model(DEVICE)\n",
    "student = load_pretrained_model(DEVICE)\n",
    "\n",
    "model_s, model_s_final = scrub(teacher, student)\n",
    "\n",
    "readout(model_s,\"scrubR\")\n",
    "readout(model_s_final,\"scrub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SezC7Ua1c6gw",
    "outputId": "7b6894c2-db6c-4e33-eb55-3a62aff95355"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import SSD.src.ssd as ssd\n",
    "\n",
    "\n",
    "def ssd_tuning(\n",
    "    model,\n",
    "    unlearning_teacher,\n",
    "    retain_train_dl,\n",
    "    retain_valid_dl,\n",
    "    forget_train_dl,\n",
    "    forget_valid_dl,\n",
    "    valid_dl,\n",
    "    dampening_constant,\n",
    "    selection_weighting,\n",
    "    full_train_dl,\n",
    "    device,\n",
    "    **kwargs,\n",
    "):\n",
    "    parameters = {\n",
    "        \"lower_bound\": 1,  # unused\n",
    "        \"exponent\": 1,  # unused\n",
    "        \"magnitude_diff\": None,  # unused\n",
    "        \"min_layer\": -1,  # -1: all layers are available for modification\n",
    "        \"max_layer\": -1,  # -1: all layers are available for modification\n",
    "        \"forget_threshold\": 1,  # unused\n",
    "        \"dampening_constant\": dampening_constant,  # Lambda from paper\n",
    "        \"selection_weighting\": selection_weighting,  # Alpha from paper\n",
    "    }\n",
    "\n",
    "    # load the trained model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    pdr = ssd.ParameterPerturber(model, optimizer, device, parameters)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    # Calculation of the forget set importances\n",
    "    sample_importances = pdr.calc_importance(forget_train_dl)\n",
    "\n",
    "    # Calculate the importances of D (see paper); this can also be done at any point before forgetting.\n",
    "    original_importances = pdr.calc_importance(full_train_dl)\n",
    "\n",
    "    # Dampen selected parameters\n",
    "    pdr.modify_weight(original_importances, sample_importances)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAFzYwnWc8fu",
    "outputId": "bfbba8b7-f665-4631-850f-8d27fe1e9299"
   },
   "outputs": [],
   "source": [
    "ssd_model = load_pretrained_model(DEVICE)\n",
    "unlearning_teacher = resnet18(weights=None, num_classes=100)\n",
    "\n",
    "kwargs = {\n",
    "    \"model\": ssd_model,\n",
    "    \"unlearning_teacher\": unlearning_teacher,\n",
    "    \"retain_train_dl\": retain_loader,\n",
    "    \"retain_valid_dl\": test_loader,\n",
    "    \"forget_train_dl\": forget_loader,\n",
    "    \"forget_valid_dl\": forget_loader,\n",
    "    \"full_train_dl\": train_loader,\n",
    "    \"valid_dl\": test_loader,\n",
    "    \"dampening_constant\": 1,\n",
    "    \"selection_weighting\": 10 * 1,\n",
    "    \"num_classes\": 10,\n",
    "    \"dataset_name\": 'Cifar10',\n",
    "    \"device\": DEVICE,\n",
    "    \"model_name\": 'resnet18',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "ssd_model = ssd_tuning(**kwargs)\n",
    "\n",
    "readout(ssd_model,\"ssd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVNkP_q1psGS"
   },
   "outputs": [],
   "source": [
    "def epoch_end(model, epoch, result):\n",
    "    print(\n",
    "        \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}\".format(\n",
    "            epoch,\n",
    "            result[\"lrs\"][-1],\n",
    "            result[\"train_loss\"],\n",
    "            result[\"Loss\"],\n",
    "            #result[\"Acc\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "def training_step(model, batch, device):\n",
    "    images, clabels = batch\n",
    "    images, clabels = images.to(device), clabels.to(device)\n",
    "    out = model(images)  # Generate predictions\n",
    "    loss = nn.functional.cross_entropy(out, clabels)  # Calculate loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    outputs = [validation_step(model, batch, device) for batch in val_loader]\n",
    "    return validation_epoch_end(model, outputs)\n",
    "\n",
    "def validation_step(model, batch, device):\n",
    "    images, clabels = batch\n",
    "    images, clabels = images.to(device), clabels.to(device)\n",
    "    out = model(images)  # Generate predictions\n",
    "    loss = nn.functional.cross_entropy(out, clabels)  # Calculate loss\n",
    "    #acc = accuracy(out, clabels)  # Calculate accuracy\n",
    "    return {\"Loss\": loss.detach()}#, \"Acc\": acc}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    batch_losses = [x[\"Loss\"] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "    #batch_accs = [x[\"Acc\"] for x in outputs]\n",
    "    #epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "    return {\"Loss\": epoch_loss.item()}#, \"Acc\": epoch_acc.item()}\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_unlearning_cycle(epochs, model, train_loader, val_loader, lr, device):\n",
    "    history = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(model, batch, device)\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.detach().cpu())\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lrs.append(get_lr(optimizer))\n",
    "\n",
    "        result = evaluate(model, val_loader, device)\n",
    "        result[\"train_loss\"] = torch.stack(train_losses).mean()\n",
    "        result[\"lrs\"] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "\n",
    "def amnesiac(\n",
    "    model,\n",
    "    unlearning_teacher,\n",
    "    retain_train_dl,\n",
    "    retain_valid_dl,\n",
    "    forget_train_dl,\n",
    "    forget_valid_dl,\n",
    "    valid_dl,\n",
    "    num_classes,\n",
    "    device,\n",
    "    **kwargs,\n",
    "):\n",
    "    unlearninglabels = list(range(num_classes))\n",
    "    unlearning_trainset = []\n",
    "\n",
    "    for x, clabel in forget_train_dl.dataset:\n",
    "        rnd = random.choice(unlearninglabels)\n",
    "        while rnd == clabel:\n",
    "            rnd = random.choice(unlearninglabels)\n",
    "        unlearning_trainset.append((x, rnd))\n",
    "\n",
    "    for x, y in retain_train_dl.dataset:\n",
    "        unlearning_trainset.append((x, y))\n",
    "\n",
    "    unlearning_train_set_dl = DataLoader(\n",
    "        unlearning_trainset, 128, pin_memory=True, shuffle=True\n",
    "    )\n",
    "\n",
    "    _ = fit_one_unlearning_cycle(\n",
    "        3, model, unlearning_train_set_dl, retain_valid_dl, device=device, lr=0.0001\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amhUxqf6uWw6",
    "outputId": "264be321-e2f5-4725-a6ed-61cdb9c6ab05"
   },
   "outputs": [],
   "source": [
    "RNG = seed_everything(SEED)\n",
    "\n",
    "amnesic_model = load_pretrained_model(DEVICE)\n",
    "unlearning_teacher = resnet18(weights=None, num_classes=100)\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "    \"model\": amnesic_model,\n",
    "    \"unlearning_teacher\": unlearning_teacher,\n",
    "    \"retain_train_dl\": retain_loader,\n",
    "    \"retain_valid_dl\": test_loader,\n",
    "    \"forget_train_dl\": forget_loader,\n",
    "    \"forget_valid_dl\": forget_loader,\n",
    "    \"full_train_dl\": train_loader,\n",
    "    \"valid_dl\": test_loader,\n",
    "    \"dampening_constant\": 1,\n",
    "    \"selection_weighting\": 10 * 1,\n",
    "    \"num_classes\": 10,\n",
    "    \"dataset_name\": 'Cifar10',\n",
    "    \"device\": DEVICE,\n",
    "    \"model_name\": 'resnet18',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "amnesiac_model = amnesiac(**kwargs)\n",
    "\n",
    "readout(amnesiac_model,\"amnesiac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNWmDG6LdEar"
   },
   "outputs": [],
   "source": [
    "with open(drive_folder+f\"results_Cifar100_SPLIT_{int(SPLIT*100)}%_SEED_{SEED}.json\", 'w') as fout:\n",
    "  json.dump(results, fout)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
