{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWHbEYoG2KOi",
    "outputId": "02971809-3e59-42b3-c208-f7967a5a965f"
   },
   "outputs": [],
   "source": [
    "drive_folder = \"Machine_Unlearning_Drive/Cifar10Results/\"\n",
    "\n",
    "ssd_folder = \"SSD/\"\n",
    "\n",
    "scrub_folder = \"SCRUB/\"\n",
    "\n",
    "github_folder = \"Machine_Unlearning/\"\n",
    "\n",
    "!pip install scikit-learn torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6idGCt52VyP",
    "outputId": "af631827-2607-448f-de73-de545a4e4094"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from Machine_Unlearning.Metrics.metrics import *\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "def seed_everything(seed):\n",
    "  RNG = torch.Generator().manual_seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  return RNG\n",
    "\n",
    "SEED = 42\n",
    "RNG = seed_everything(SEED)\n",
    "SPLIT = 0.15\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "FAerSXaSB_Rl",
    "outputId": "94b7c940-e314-4484-a296-4a839243303a"
   },
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "import torchvision.datasets as dts\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plot\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "train_set = dts.CIFAR10(root='./data', download=True, train=True, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "\n",
    "held_out = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=test_transform\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "\n",
    "GEN1 = torch.Generator().manual_seed(42)\n",
    "\n",
    "retain_set, forget_set = torch.utils.data.random_split(train_set,[1-SPLIT,SPLIT],GEN1)\n",
    "RNG = seed_everything(1337)\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=256, shuffle=True, num_workers=2 , generator=RNG\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=256, shuffle=True, num_workers=2, generator=RNG\n",
    ")\n",
    "\n",
    "model = models.resnet18(weights=None, num_classes=10)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.title(\"Sample images from Cifar10 dataset\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDSlsKVCmhJA"
   },
   "outputs": [],
   "source": [
    "def accuracy(net, loader):\n",
    "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i,(inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        #print(i)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjxV5iqd17wS"
   },
   "outputs": [],
   "source": [
    "def readout(model,name):\n",
    "  RNG = seed_everything(SEED)\n",
    "  test_entropies = compute_entropy(model, test_loader)\n",
    "  retain_entropies = compute_entropy(model, retain_loader)\n",
    "  forget_entropies = compute_entropy(model, forget_loader)\n",
    "\n",
    "\n",
    "  results[f\"test_entropies_{name}\"] = test_entropies.tolist()\n",
    "  results[f\"retain_entropies_{name}\"] = retain_entropies.tolist()\n",
    "  results[f\"forget_entropies_{name}\"] = forget_entropies.tolist()\n",
    "\n",
    "  test_losses = compute_losses(model, test_loader)\n",
    "  retain_losses = compute_losses(model, retain_loader)\n",
    "  forget_losses = compute_losses(model, forget_loader)\n",
    "\n",
    "  results[f\"test_losses_{name}\"] = test_losses.tolist()\n",
    "  results[f\"retain_losses_{name}\"] = retain_losses.tolist()\n",
    "  results[f\"forget_losses_{name}\"] = forget_losses.tolist()\n",
    "\n",
    "  # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
    "  gen = np.random.default_rng(1)\n",
    "  if len(test_losses) > len(forget_losses):\n",
    "    gen.shuffle(test_losses)\n",
    "    test_losses = test_losses[: len(forget_losses)]\n",
    "  else:\n",
    "    gen.shuffle(forget_losses)\n",
    "    forget_losses = forget_losses[: len(test_losses)]\n",
    "    # make sure we have a balanced dataset for the MIA\n",
    "  assert len(test_losses) == len(forget_losses)\n",
    "\n",
    "  samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
    "  labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
    "\n",
    "  mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "\n",
    "  print(\n",
    "      f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "  )\n",
    "\n",
    "  results[f\"MIA_losses_{name}\"] = mia_scores.mean()\n",
    "\n",
    "  gen = np.random.default_rng(1)\n",
    "  if len(test_entropies) > len(forget_entropies):\n",
    "    gen.shuffle(test_entropies)\n",
    "    test_entropies = test_entropies[: len(forget_entropies)]\n",
    "  else:\n",
    "    gen.shuffle(forget_entropies)\n",
    "    forget_entropies = forget_entropies[: len(test_entropies)]\n",
    "    # make sure we have a balanced dataset for the MIA\n",
    "  assert len(test_entropies) == len(forget_entropies)\n",
    "\n",
    "  samples_mia = np.concatenate((test_entropies, forget_entropies)).reshape((-1, 1))\n",
    "  labels_mia = [0] * len(test_entropies) + [1] * len(forget_entropies)\n",
    "\n",
    "  mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "\n",
    "  print(\n",
    "      f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\"\n",
    "  )\n",
    "\n",
    "  results[f\"MIA_entropies_{name}\"] = mia_scores.mean()\n",
    "\n",
    "  results[f\"train_accuracy_{name}\"] = accuracy(model, retain_loader)\n",
    "  results[f\"test_accuracy_{name}\"] = accuracy(model, test_loader)\n",
    "  results[f\"forget_accuracy_{name}\"] = accuracy(model, forget_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkLy-EqWEupw",
    "outputId": "b3d783cc-0413-4e29-d7a6-df0c5a2b4c2b"
   },
   "outputs": [],
   "source": [
    "#This model has been trained using SGD with a learning rate of 0.1, momentum of 0.9 and weight decay of 5e-4.\n",
    "\n",
    "train_loader = retain_loader\n",
    "\n",
    "    \n",
    "numepchs = 50\n",
    "lr = 0.1\n",
    "criter = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr,momentum = 0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=1.0, end_factor=0.001, total_iters=numepchs)\n",
    "nttlstps = len(train_loader)\n",
    "model.train()\n",
    "for epoch in range(numepchs):\n",
    "    for x, (imgs, lbls) in enumerate(train_loader):\n",
    "         imgs , lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "\n",
    "         outp = model(imgs)\n",
    "         losses = criter(outp, lbls)\n",
    "\n",
    "         optim.zero_grad()\n",
    "         losses.backward()\n",
    "         optim.step()\n",
    "         if x % 100 == 0:\n",
    "           print (f'Epochs [{epoch+1}/{numepchs}], Step[{x+1}/{nttlstps}], Losses: {losses.item():.4f}')\n",
    "    #train_acc = accuracy(model, train_loader) * 100\n",
    "    #test_acc = accuracy(model, test_loader) * 100\n",
    "    #print(f'Epoch {epoch+50} Train Acc:{train_acc:0.1f} Test_acc:{test_acc:0.1f}')\n",
    "    #torch.save(model.state_dict(),f\"./checkpoint_cifar10_no_aug_epoch{epoch+50}_{train_acc:0.1f}_{test_acc:0.1f}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "2MzQpN3H20WZ",
    "outputId": "6cec630b-72b7-46b2-820e-a1c70bed0278"
   },
   "outputs": [],
   "source": [
    "readout(model,\"retrained\")\n",
    "with open(drive_folder+f\"results_Cifar10_SPLIT_{int(SPLIT*100)}%_SEED_{SEED}_retrained.json\", 'w') as fout:\n",
    "  json.dump(results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
